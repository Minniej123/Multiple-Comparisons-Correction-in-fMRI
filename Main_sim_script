#in this simulation we explore both FDR methods applied to climate as well as classical methods applied to fMRI.
#We compare a series of metrics, including DICE coefficients, blob size, number of sigificant pixels, convergence rates along with Power FDR and FWER.
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import ndimage, stats
from tqdm.auto import tqdm

#The set up:
#We use a 2D simulation as 3D is far too complicated, we create a signal-Noise model to simulate our results for our methods

#setting up our simulation
#reproducability
rng_master = np.random.default_rng(0)
rng = np.random.default_rng(7)
#grid
v = 200 # number of voxels in each dimension
Nx, Ny = v, v# 200 x200 grid
Y, X = np.mgrid[0:Ny, 0:Nx]

#Hyperparams:
#we want to vary the number of subjects
#can vary signal strength
n_blobs = 3 #changing number of blobs

alpha = 0.05 #signif level stays the same throughout our simulations
q_fdr=0.05 #We use this in our FDR approaches, stays the same throughout our simulations
#smoothness
FWHM_SIGNAL_FIXED = 18 #I found that changing this changed the performance of cluster based, smaller fwhm led to higher FWER in cluster
#convert to gaussian
SIGMA_SIGNAL_FIXED = float(FWHM_SIGNAL_FIXED) / np.sqrt(8.0 * np.log(2.0))
n_runs = 200 #could vary this, Monte Carlo runs per scenario
p0_cluster = 0.001 # cluster-forming p threshold (one-sided)
n_perm_cluster = 1000 #permutations per run

#by including an analysis mask we ensure everything we look at is within the area of interest
analysis_mask = np.ones((Ny, Nx), dtype=bool)
_STRUCTURE_2D = np.ones((3, 3), dtype=int)

FRAC_OF_PEAK = 0.20 #we use this in our buffer
DILATE_ITERS = 10 #this is for dilation

#generating signal:
def fwhm_to_tau(fwhm: float) -> float:
    return float(fwhm) / np.sqrt(8.0 * np.log(2.0))

def gaussian_blob(x0, y0, amp, sigma_s):
    return amp * np.exp(-((X - x0) ** 2 + (Y - y0) ** 2) / (2.0 * sigma_s ** 2))

def make_signal_map(n_blobs, amp, sigma_s, radius=60, alt_sign=False):
    S = np.zeros((Ny, Nx), dtype=float)
    if n_blobs <= 0 or amp == 0.0:
        return S

    cx, cy = Nx / 2.0, Ny / 2.0
    if n_blobs == 1:
        centers = [(cx, cy)]
    else:
        angles = np.linspace(0, 2 * np.pi, n_blobs, endpoint=False)
        centers = [(cx + radius * np.cos(a), cy + radius * np.sin(a)) for a in angles]

    for idx, (x0, y0) in enumerate(centers):
        sign = -1.0 if (alt_sign and (idx % 2 == 1)) else 1.0
        S += gaussian_blob(x0, y0, amp=sign * amp, sigma_s=sigma_s)

    return S
#since we are only interested in areas of activation, we can consider one-sided only
def make_signal_mask(S, frac_of_peak=FRAC_OF_PEAK):
    peak = float(np.max(S))
    if peak <= 0:
        return np.zeros_like(S, dtype=bool)
    return (S >= (frac_of_peak * peak)) & analysis_mask

def make_eval_masks(core_mask, dilate_iters=DILATE_ITERS):
    core = core_mask.astype(bool) & analysis_mask #true activation mask
    if not np.any(core):
        null = analysis_mask.copy() #null region
        buf = np.zeros_like(core, dtype=bool) #buffer ring, we ignore in FP counting
        return core, null, buf

    core_dil = ndimage.binary_dilation(core, structure=_STRUCTURE_2D, iterations=int(dilate_iters))
    null = analysis_mask & (~core_dil)
    buf = analysis_mask & (~core) & (~null)
    return core, null, buf

#one sided p-val
def compute_t_p(images, eps=1e-8):
    images = np.asarray(images)
    n = images.shape[0]
    df = n - 1
    mean = images.mean(axis=0)
    sd = images.std(axis=0, ddof=1)
    tmap = mean / np.maximum(sd / np.sqrt(n), eps)
    pmap = stats.t.sf(tmap, df=df)
    pmap = np.clip(pmap, 1e-300, 1.0)
    return tmap, pmap

#cluster perm, sign flip
def max_cluster_size_one_sided(tmap, t0, mask):
    supra = (tmap >= t0) & mask
    lab, nlab = ndimage.label(supra, structure=_STRUCTURE_2D)
    if nlab == 0:
        return 0
    sizes = ndimage.sum(np.ones_like(tmap, int), labels=lab, index=np.arange(1, nlab + 1))
    return int(np.max(sizes))

def cluster_perm_onesample_one_sided(images, p0_cluster, alpha, n_perm, mask, rng):
    images = np.asarray(images)
    n = images.shape[0]
    df = n - 1

    t_obs, _ = compute_t_p(images)

    #cluster forming threshold:
    t0 = float(stats.t.isf(p0_cluster, df=df))

    null_max = np.empty(n_perm, dtype=int)
    for b in range(n_perm):
        signs = rng.choice([-1.0, 1.0], size=n)[:, None, None]
        t_perm, _ = compute_t_p(images * signs)
        null_max[b] = max_cluster_size_one_sided(t_perm, t0, mask=mask)

    supra = (t_obs >= t0) & mask
    lab, nlab = ndimage.label(supra, structure=_STRUCTURE_2D)

    sig = np.zeros_like(supra, dtype=bool)
    if nlab == 0:
        return sig, t0
    sizes = ndimage.sum(np.ones_like(t_obs, int), labels=lab, index=np.arange(1, nlab + 1)).astype(int)
    for k, s in enumerate(sizes, start=1):
        p_cluster = (1.0 + np.sum(null_max >= s)) / (n_perm + 1.0)
        if p_cluster <= alpha:
            sig[lab == k] = True
    return sig, t0

#metrics
def dice(mask, truth):
    inter = int(np.sum(mask & truth))
    denom = int(np.sum(mask) + np.sum(truth))
    return 2.0 * inter / max(denom, 1)

def cluster_level_counts(mask_sig, core_mask, null_mask):
    sig = mask_sig.astype(bool) & analysis_mask
    lab, nlab = ndimage.label(sig, structure=_STRUCTURE_2D)
    R_cluster = int(nlab)
    if nlab == 0:
        return R_cluster, 0

    core = core_mask.astype(bool) & analysis_mask
    null = null_mask.astype(bool) & analysis_mask

    V_cluster = 0
    for k in range(1, nlab + 1):
        cl = (lab == k)
        touches_core = bool(np.any(cl & core))
        touches_null = bool(np.any(cl & null))
        if touches_null and (not touches_core):
            V_cluster += 1
    return R_cluster, int(V_cluster)

#we create one evaluate method function to compute all metrics once:
def evaluate_method(mask_sig, core_mask, null_mask, report_fdr=True, report_cluster=False):
    sig = mask_sig.astype(bool) & analysis_mask
    core = core_mask.astype(bool) & analysis_mask
    null = null_mask.astype(bool) & analysis_mask

    TP = int(np.sum(sig & core)) #True pos
    FP = int(np.sum(sig & null)) #false pos
    FN = int(np.sum((~sig) & core)) #false neg

    R = TP + FP
    Power = TP / max(TP + FN, 1)
    FDP_run = (FP / max(R, 1)) if report_fdr else np.nan

    out = {
        "Power": float(Power),
        "Dice": float(dice(sig, core)),
        "n_sig_pixels": int(np.sum(sig)),
        "FDP_run": float(FDP_run) if report_fdr else np.nan,
        "V": int(FP) if report_fdr else np.nan,
        "R": int(R) if report_fdr else np.nan,
        "FWER_vox": float(1.0 if FP > 0 else 0.0),
        "R_cluster": np.nan,
        "V_cluster": np.nan,
        "FDP_cluster_run": np.nan,
        "FWER_cluster": np.nan,
    }

    if report_cluster:
        Rcl, Vcl = cluster_level_counts(sig, core, null)
        out["R_cluster"] = int(Rcl)
        out["V_cluster"] = int(Vcl)
        out["FDP_cluster_run"] = float(Vcl / max(Rcl, 1))
        out["FWER_cluster"] = float(1.0 if Vcl > 0 else 0.0)

    return out

#we define helper function for simulation
def simulate_images(S, n_subj, sigma_N, tau, rng):
    imgs = np.empty((n_subj, Ny, Nx), dtype=float)
    for i in range(n_subj):
        white = rng.normal(0.0, sigma_N, size=(Ny, Nx))
        smooth = ndimage.gaussian_filter(white, sigma=tau, mode="reflect")
        imgs[i] = S + smooth
    return imgs

def run_one(images, core_mask, null_mask, rng_run):
    tmap, pmap = compute_t_p(images)

    p_vec = pmap[analysis_mask].ravel()
    p_sorted = np.sort(p_vec)
    m = p_sorted.size

    metrics = {}

    #bonferroni
    bonf = np.zeros_like(pmap, dtype=bool)
    bonf[analysis_mask] = pmap[analysis_mask] <= (alpha / m)
    metrics["Bonf"] = evaluate_method(bonf, core_mask, null_mask, report_fdr=False)

    #BH
    bh_line = (np.arange(1, m + 1) / m) * q_fdr
    bh_ok = p_sorted <= bh_line
    if np.any(bh_ok):
        pcrit_bh = float(p_sorted[np.max(np.where(bh_ok)[0])])
    else:
        pcrit_bh = -np.inf
    bh = np.zeros_like(pmap, dtype=bool)
    if np.isfinite(pcrit_bh):
        bh[analysis_mask] = pmap[analysis_mask] <= pcrit_bh
    metrics["BH"] = evaluate_method(bh, core_mask, null_mask, report_fdr=True)
    metrics["BH"]["pcrit"] = float(pcrit_bh)

    #BY
    c_m = np.sum(1.0 / np.arange(1, m + 1))
    by_line = (np.arange(1, m + 1) / m) * (q_fdr / c_m)
    by_ok = p_sorted <= by_line
    if np.any(by_ok):
        pcrit_by = float(p_sorted[np.max(np.where(by_ok)[0])])
    else:
        pcrit_by = -np.inf
    by = np.zeros_like(pmap, dtype=bool)
    if np.isfinite(pcrit_by):
        by[analysis_mask] = pmap[analysis_mask] <= pcrit_by
    metrics["BY"] = evaluate_method(by, core_mask, null_mask, report_fdr=True)
    metrics["BY"]["pcrit"] = float(pcrit_by)

    #BKY
    q0 = q_fdr / (1.0 + q_fdr)
    bky1_line = (np.arange(1, m + 1) / m) * q0
    bky1_ok = p_sorted <= bky1_line
    if np.any(bky1_ok):
        pcrit1 = float(p_sorted[np.max(np.where(bky1_ok)[0])])
    else:
        pcrit1 = -np.inf
    R1 = int(np.sum(p_vec <= pcrit1)) if np.isfinite(pcrit1) else 0
    m0_hat = max(1, m - R1)
    q_star = min(1.0, q_fdr * (m / m0_hat))

    bky2_line = (np.arange(1, m + 1) / m) * q_star
    bky2_ok = p_sorted <= bky2_line
    if np.any(bky2_ok):
        pcrit_bky = float(p_sorted[np.max(np.where(bky2_ok)[0])])
    else:
        pcrit_bky = -np.inf

    bky = np.zeros_like(pmap, dtype=bool)
    if np.isfinite(pcrit_bky):
        bky[analysis_mask] = pmap[analysis_mask] <= pcrit_bky

    metrics["BKY"] = evaluate_method(bky, core_mask, null_mask, report_fdr=True)
    metrics["BKY"]["pcrit"] = float(pcrit_bky)

    #cluster permutation
    rng_perm = np.random.default_rng(rng_run.integers(0, 2**32 - 1))
    clus, t0 = cluster_perm_onesample_one_sided(
        images,
        p0_cluster=p0_cluster,
        alpha=alpha,
        n_perm=n_perm_cluster,
        mask=analysis_mask,
        rng=rng_perm,
    )
    metrics["ClusterPerm"] = evaluate_method(clus, core_mask, null_mask, report_fdr=False, report_cluster=True)
    metrics["ClusterPerm"]["t0"] = float(t0)

    debug_last = {
        "pmap": pmap,
        "tmap": tmap,
        "t0": float(t0),
        "sig_masks": {
            "Bonf": bonf,
            "BH": bh,
            "BY": by,
            "BKY": bky,
            "ClusterPerm": clus,
        },
    }
    return metrics, debug_last

#visual
def plot_last_run(sc_name, S, core_mask, null_mask, debug_last):
    pmap = debug_last["pmap"]
    tmap = debug_last["tmap"]
    masks = debug_last["sig_masks"]
    zmap = stats.norm.isf(np.clip(pmap, 1e-300, 1.0))
    zmap_signed = np.sign(tmap) * zmap  #visual contrast

    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    ax = axes.ravel()

    im0 = ax[0].imshow(S, cmap="coolwarm")
    ax[0].set_title(f"{sc_name}: True Signal S")
    ax[0].set_xticks([]); ax[0].set_yticks([])
    plt.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)

    im1 = ax[1].imshow(zmap_signed, cmap="coolwarm", vmin=-6, vmax=6)
    ax[1].set_title("Signed Z-ish map (from one-sided p)")
    ax[1].set_xticks([]); ax[1].set_yticks([])
    plt.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)

    # show truth and null
    ax[2].imshow(core_mask, cmap="Greens", alpha=0.9)
    ax[2].set_title("Truth (core)")
    ax[2].set_xticks([]); ax[2].set_yticks([])

    ax[3].imshow(null_mask, cmap="Blues", alpha=0.9)
    ax[3].set_title("Null region (for FP)")
    ax[3].set_xticks([]); ax[3].set_yticks([])

    def show_mask(ax_, mask, title):
        ax_.imshow(mask, cmap="Reds", alpha=0.85)
        ax_.set_title(title)
        ax_.set_xticks([])
        ax_.set_yticks([])

    show_mask(ax[4], masks["Bonf"], "Bonferroni")
    show_mask(ax[5], masks["BH"], "BH")
    show_mask(ax[6], masks["BY"], "BY")
    show_mask(ax[7], masks["BKY"], "BKY")

    plt.tight_layout()
    plt.show()

    fig2, ax2 = plt.subplots(1, 1, figsize=(5, 5))
    ax2.imshow(masks["ClusterPerm"], cmap="Reds", alpha=0.85)
    ax2.set_title(f"ClusterPerm one-sided (t0â‰ˆ{debug_last['t0']:.2f})")
    ax2.set_xticks([]); ax2.set_yticks([])
    plt.tight_layout()
    plt.show()

#scenarios
scenarios = [
    {"name": "NULL",    "n_subj": 30, "fwhm":  8, "sigma_N": 1.0, "n_blobs": 0, "amp": 0.0},
    {"name": "typical", "n_subj": 30, "fwhm":  8, "sigma_N": 1.0, "n_blobs": 2, "amp": 1.0},
]

methods = ["Bonf", "BH", "BY", "BKY", "ClusterPerm"]

all_rows = []
print(f"Starting simulation with n_runs={n_runs}, n_perm_cluster={n_perm_cluster}")
runs_rows = []

for sc in scenarios:
    sc_name = sc["name"]
    n_subj = sc["n_subj"]
    fwhm = sc["fwhm"]
    sigma_N = sc["sigma_N"]
    n_blobs = sc["n_blobs"]
    amp = sc["amp"]

    tau = fwhm_to_tau(fwhm)
    sigma_S = SIGMA_SIGNAL_FIXED

    print(
        f"\n=== Scenario: {sc_name} | n_subj={n_subj}, FWHM={fwhm} (tau={tau:.2f}), "
        f"sigma_N={sigma_N}, n_blobs={n_blobs}, amp={amp} ==="
    )

    S = make_signal_map(n_blobs=n_blobs, amp=amp, sigma_s=sigma_S, radius=60, alt_sign=False)

    core_truth = make_signal_mask(S, frac_of_peak=FRAC_OF_PEAK) if (amp != 0.0 and n_blobs > 0) else np.zeros((Ny, Nx), bool)
    core_mask, null_mask, _ = make_eval_masks(core_truth, dilate_iters=DILATE_ITERS)

    store = {m: [] for m in methods}
    t0_list = []
    last_debug = None

    t_start = time.time()
    for run_idx in tqdm(range(n_runs), desc=f"{sc_name} Monte Carlo", total=n_runs):
        rng_run = np.random.default_rng(rng_master.integers(0, 2**32 - 1))
        images = simulate_images(S, n_subj=n_subj, sigma_N=sigma_N, tau=tau, rng=rng_run)

        metrics, debug_last = run_one(images, core_mask, null_mask, rng_run)

        #we record cluster threshold for this run
        t0_list.append(debug_last["t0"])
        run_id = run_idx

        #here we save per run metrics for each method
        for mth in methods:
            store[mth].append(metrics[mth])

            row_run = {"scenario": sc_name, "run": run_id, "method": mth}
            row_run.update(metrics[mth])
            runs_rows.append(row_run)

        last_debug = debug_last


    #visual
    plot_last_run(sc_name, S, core_mask, null_mask, last_debug)

    #metrics together
    for mth in methods:
        df_runs = pd.DataFrame(store[mth])
        mean_metrics = df_runs.mean(numeric_only=True).to_dict()

        if mth in ["BH", "BY", "BKY"]:
            fdr_uncond = float(np.mean(df_runs["FDP_run"]))
            mask_Rpos = df_runs["R"] > 0
            fdr_cond = float(np.mean(df_runs.loc[mask_Rpos, "FDP_run"])) if np.any(mask_Rpos) else np.nan
        else:
            fdr_uncond = np.nan
            fdr_cond = np.nan

        row = {
            "scenario": sc_name,
            "method": mth,
            "n_subj": n_subj,
            "FWHM": fwhm,
            "tau": float(tau),
            "sigma_N": float(sigma_N),
            "n_blobs": n_blobs,
            "amp": amp,
            "sigma_S": float(sigma_S),
            "p0_cluster": p0_cluster,
            "n_perm_cluster": n_perm_cluster,
            "t0_mean": float(np.mean(t0_list)),
            "alpha_FWER": float(alpha),
            "q_FDR": float(q_fdr),
            "FDR_emp_uncond": fdr_uncond,
            "FDR_emp_cond_Rgt0": fdr_cond,
        }
        row.update(mean_metrics)
        all_rows.append(row)

summary = pd.DataFrame(all_rows).round(3)
print("\n=== COMBINED SUMMARY (all scenarios) ===")
print(summary.to_string(index=False))

runs_df = pd.DataFrame(runs_rows)
print("Per-run dataframe shape:", runs_df.shape)
display(runs_df.head())


out_csv = "simulation_summary_one_sided.csv"
summary.to_csv(out_csv, index=False)
print(f"\nSaved {out_csv}")
